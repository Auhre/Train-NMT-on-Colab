{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# PRELIMINARY"
      ],
      "metadata": {
        "id": "ByGzJY9iH0gg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies, restart and run all after running this cell\n",
        "!pip install tensorflow==2.5.2"
      ],
      "metadata": {
        "id": "jgdFk4Opw6mz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LOAD MODULES AND YOUR DATASET SECTION"
      ],
      "metadata": {
        "id": "up83n27WG5j0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import os\n",
        "import json\n",
        "import io\n",
        "from collections import Counter\n",
        "from tensorflow.python.keras.models import load_model\n",
        "from keras_preprocessing.text import tokenizer_from_json\n",
        "from sklearn.model_selection import train_test_split\n",
        "from string import punctuation\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.python.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "cXkK2LFNvBLZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/researchParallelCorpus.csv')"
      ],
      "metadata": {
        "id": "4vCGcD1CHE3F"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CLASS AND FUNCTIONS SECTION"
      ],
      "metadata": {
        "id": "Vk_ubHeKHSlq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "n4vflp19bKmG"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "\n",
        "\n",
        "class AttentionLayer(Layer):\n",
        "    \"\"\"\n",
        "    This class implements Bahdanau attention (https://arxiv.org/pdf/1409.0473.pdf).\n",
        "    There are three sets of weights introduced W_a, U_a, and V_a\n",
        "     \"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(AttentionLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert isinstance(input_shape, list)\n",
        "\n",
        "        self.W_a = self.add_weight(name='W_a',\n",
        "                                   shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.U_a = self.add_weight(name='U_a',\n",
        "                                   shape=tf.TensorShape((input_shape[1][2], input_shape[0][2])),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.V_a = self.add_weight(name='V_a',\n",
        "                                   shape=tf.TensorShape((input_shape[0][2], 1)),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "\n",
        "        super(AttentionLayer, self).build(input_shape)  \n",
        "\n",
        "    def call(self, inputs, verbose=False):\n",
        "        \"\"\"\n",
        "        inputs: [encoder_output_sequence, decoder_output_sequence]\n",
        "        \"\"\"\n",
        "        assert type(inputs) == list\n",
        "        encoder_out_seq, decoder_out_seq = inputs\n",
        "        if verbose:\n",
        "            print('encoder_out_seq>', encoder_out_seq.shape)\n",
        "            print('decoder_out_seq>', decoder_out_seq.shape)\n",
        "\n",
        "        def energy_step(inputs, states):\n",
        "            \"\"\" Step function for computing energy for a single decoder state \"\"\"\n",
        "\n",
        "            assert_msg = \"States must be a list. However states {} is of type {}\".format(states, type(states))\n",
        "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
        "\n",
        "            \"\"\" Some parameters required for shaping tensors\"\"\"\n",
        "            batch_size = encoder_out_seq.shape[0]\n",
        "            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]\n",
        "            de_hidden = inputs.shape[-1]\n",
        "\n",
        "            \"\"\" Computing S.Wa where S=[s0, s1, ..., si]\"\"\"\n",
        "\n",
        "            reshaped_enc_outputs = K.reshape(encoder_out_seq, (-1, en_hidden))\n",
        "\n",
        "            W_a_dot_s = K.reshape(K.dot(reshaped_enc_outputs, self.W_a), (-1, en_seq_len, en_hidden))\n",
        "            if verbose:\n",
        "                print('wa.s>',W_a_dot_s.shape)\n",
        "\n",
        "            \"\"\" Computing hj.Ua \"\"\"\n",
        "            U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1)  \n",
        "            if verbose:\n",
        "                print('Ua.h>',U_a_dot_h.shape)\n",
        "\n",
        "            \"\"\" tanh(S.Wa + hj.Ua) \"\"\"\n",
        "\n",
        "            reshaped_Ws_plus_Uh = K.tanh(K.reshape(W_a_dot_s + U_a_dot_h, (-1, en_hidden)))\n",
        "            if verbose:\n",
        "                print('Ws+Uh>', reshaped_Ws_plus_Uh.shape)\n",
        "\n",
        "            \"\"\" softmax(va.tanh(S.Wa + hj.Ua)) \"\"\"\n",
        "\n",
        "            e_i = K.reshape(K.dot(reshaped_Ws_plus_Uh, self.V_a), (-1, en_seq_len))\n",
        "\n",
        "            e_i = K.softmax(e_i)\n",
        "\n",
        "            if verbose:\n",
        "                print('ei>', e_i.shape)\n",
        "\n",
        "            return e_i, [e_i]\n",
        "\n",
        "        def context_step(inputs, states):\n",
        "            \"\"\" Step function for computing ci using ei \"\"\"\n",
        "\n",
        "            c_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)\n",
        "            if verbose:\n",
        "                print('ci>', c_i.shape)\n",
        "            return c_i, [c_i]\n",
        "\n",
        "        def create_inital_state(inputs, hidden_size):\n",
        "\n",
        "            fake_state = K.zeros_like(inputs)\n",
        "            fake_state = K.sum(fake_state, axis=[1, 2]) \n",
        "            fake_state = K.expand_dims(fake_state) \n",
        "            fake_state = K.tile(fake_state, [1, hidden_size]) \n",
        "            return fake_state\n",
        "\n",
        "        fake_state_c = create_inital_state(encoder_out_seq, encoder_out_seq.shape[-1])\n",
        "        fake_state_e = create_inital_state(encoder_out_seq, encoder_out_seq.shape[1]) \n",
        "\n",
        "        \"\"\" Computing energy outputs \"\"\"\n",
        "        \n",
        "        last_out, e_outputs, _ = K.rnn(\n",
        "            energy_step, decoder_out_seq, [fake_state_e],\n",
        "        )\n",
        "\n",
        "        \"\"\" Computing context vectors \"\"\"\n",
        "        last_out, c_outputs, _ = K.rnn(\n",
        "            context_step, e_outputs, [fake_state_c],\n",
        "        )\n",
        "\n",
        "        return c_outputs, e_outputs\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        \"\"\" Outputs produced by the layer \"\"\"\n",
        "        return [\n",
        "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),\n",
        "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1]))\n",
        "        ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9-LZp1nSbME9"
      },
      "outputs": [],
      "source": [
        "def save_model(dir_hash,model_dict,full_model,encoder_model,decoder_model,source_tokenizer,target_tokenizer):\n",
        "    if not os.path.exists('h5.models/' + dir_hash):\n",
        "        os.makedirs('h5.models/' + dir_hash)\n",
        "    with open('h5.models/' + dir_hash + \"/model_params.json\", 'w') as f:\n",
        "        f.write(json.dumps(model_dict))\n",
        "    tokenizer_json=source_tokenizer.to_json()\n",
        "    with io.open('h5.models/'+dir_hash+'/source_tokenizer.json','w',encoding='utf-8') as f:\n",
        "        f.write(json.dumps(tokenizer_json,ensure_ascii=False))\n",
        "    tokenizer_json=target_tokenizer.to_json()\n",
        "    with io.open('h5.models/'+dir_hash+'/target_tokenizer.json', 'w', encoding='utf-8') as f:\n",
        "        f.write(json.dumps(tokenizer_json, ensure_ascii=False))\n",
        "    full_model.save('h5.models/'+dir_hash+'/full_model.h5')\n",
        "    encoder_model.save('h5.models/'+dir_hash+'/encoder_model.h5')\n",
        "    decoder_model.save('h5.models/'+dir_hash+'/decoder_model.h5')\n",
        "\n",
        "def load_saved_model(dir_hash):\n",
        "    with open('h5.models/'+dir_hash+'/model_params.json','r') as f:\n",
        "        for line in f:\n",
        "            data=json.loads(line)\n",
        "    with open('h5.models/'+dir_hash+'/source_tokenizer.json',encoding='utf-8') as f:\n",
        "        temp=json.load(f)\n",
        "        source_tokenizer=tokenizer_from_json(temp)\n",
        "    with open('h5.models/'+dir_hash+'/target_tokenizer.json',encoding='utf-8') as f:\n",
        "        temp=json.load(f)\n",
        "        target_tokenizer=tokenizer_from_json(temp)\n",
        "    full_model=load_model('h5.models/'+dir_hash+'/full_model.h5', custom_objects={'AttentionLayer': AttentionLayer})\n",
        "    encoder_model=load_model('h5.models/'+dir_hash+'/encoder_model.h5', custom_objects={'AttentionLayer': AttentionLayer})\n",
        "    decoder_model=load_model('h5.models/'+dir_hash+'/decoder_model.h5', custom_objects={'AttentionLayer': AttentionLayer})\n",
        "    return data,source_tokenizer,target_tokenizer,full_model,encoder_model,decoder_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "1AbfJb_MbNlm"
      },
      "outputs": [],
      "source": [
        "def get_data(data_file):\n",
        "\n",
        "    std = data_file['Standard']\n",
        "    n_std = data_file['Non-Standard']\n",
        "\n",
        "    std = std.str.lower()\n",
        "    std = std.str.replace('\\W', ' ')\n",
        "    std = std.dropna()\n",
        "\n",
        "    n_std = n_std.str.lower()\n",
        "    n_std = n_std.str.replace('\\W', ' ')\n",
        "    n_std = n_std.dropna()\n",
        "  \n",
        "    # Swap here to alter model training \n",
        "    source_lang_text_data = n_std.values\n",
        "    target_lang_text_data = std.values \n",
        "\n",
        "    source_lang_text_data=[item.rstrip() for item in source_lang_text_data]\n",
        "    target_lang_text_data=[item.rstrip() for item in target_lang_text_data]\n",
        "    target_lang_text_data=['sentencestart '+sent[:-1]+' sentenceend .' if sent.endswith('.') else 'sentencestart '+sent+' sentenceend .' for sent in target_lang_text_data]\n",
        "\n",
        "    print(\"Length of text {}\".format(len(source_lang_text_data)))\n",
        "    return source_lang_text_data,target_lang_text_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "UMpNBH_LbP20"
      },
      "outputs": [],
      "source": [
        "def build_tokenizer_and_split_text(data_file,src_min_words=1,tgt_min_words=1):\n",
        "    source_lang_text_data,target_lang_text_data=get_data(data_file)\n",
        "\n",
        "    split_condition = re.compile(r\"\\w+|[^\\w\\s]\", re.UNICODE)\n",
        "\n",
        "    source_words = Counter()\n",
        "    for sentence in source_lang_text_data:\n",
        "        words=[word for word in split_condition.findall(sentence.lower()) if word not in punctuation]\n",
        "        for word in words:\n",
        "            source_words[word] += 1\n",
        "    target_words = Counter()\n",
        "    for sentence in target_lang_text_data:\n",
        "        words = [word for word in split_condition.findall(sentence.lower()) if word not in punctuation]\n",
        "        for word in words:\n",
        "            target_words[word] += 1\n",
        "    print(\"Total unique words in source lang: \"+str(len(source_words)))\n",
        "    print(\"Total unique words in target lang: \"+str(len(target_words)))\n",
        "    source_vocab_count=0\n",
        "    for word in source_words:\n",
        "        if source_words[word]>=src_min_words:source_vocab_count+=1\n",
        "    target_vocab_count=0\n",
        "    for word in target_words:\n",
        "        if target_words[word]>=tgt_min_words:target_vocab_count+=1\n",
        "    print(\"Total unique source words with min count \"+str(src_min_words)+': '+str(source_vocab_count))\n",
        "    print(\"Total unique target words with min count \"+str(tgt_min_words)+': '+str(target_vocab_count))\n",
        "\n",
        "    source_tokenizer=keras.preprocessing.text.Tokenizer(num_words=source_vocab_count+1,oov_token='UNK')\n",
        "    source_tokenizer.fit_on_texts(source_lang_text_data)\n",
        "    target_tokenizer=keras.preprocessing.text.Tokenizer(num_words=target_vocab_count+1,oov_token='UNK')\n",
        "    target_tokenizer.fit_on_texts(target_lang_text_data)\n",
        "    src_train, src_test, tgt_train, tgt_test = train_test_split(source_lang_text_data, target_lang_text_data, test_size=0.1)\n",
        "    src_train, src_cv, tgt_train, tgt_cv = train_test_split(src_train, tgt_train,test_size=0.1)\n",
        "    return src_train,src_cv,src_test,tgt_train,tgt_cv,tgt_test,source_tokenizer,target_tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "M46eMdoUbTul"
      },
      "outputs": [],
      "source": [
        "from tensorflow.python.keras.layers import Input,Embedding,GRU,Dense,TimeDistributed,Bidirectional,Concatenate\n",
        "from tensorflow.python.keras.models import Model\n",
        "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "def define_nmt(hidden_size,embedding_dim,source_lang_timesteps,source_lang_vocab_size,target_lang_timesteps,target_lang_vocab_size,dropout):\n",
        "\n",
        "    encoder_inputs=Input(shape=(source_lang_timesteps,),name='encoder_inputs')\n",
        "    decoder_inputs=Input(shape=(target_lang_timesteps-1,),name='decoder_inputs')\n",
        "\n",
        "    encoder_embedding_layer = Embedding(input_dim=source_lang_vocab_size, output_dim=embedding_dim)\n",
        "    encoder_embedded = encoder_embedding_layer(encoder_inputs)\n",
        "    decoder_embedding_layer = Embedding(input_dim=target_lang_vocab_size, output_dim=embedding_dim)\n",
        "    decoder_embedded = decoder_embedding_layer(decoder_inputs)\n",
        "\n",
        "    encoder_gru1 = GRU(2 * hidden_size, return_sequences=True, return_state=True, name='encoder_gru1')\n",
        "    encoder_out1, encoder_state1 = encoder_gru1(encoder_embedded)\n",
        "    encoder_gru2 = GRU(2 * hidden_size, return_sequences=True, return_state=True, name='encoder_gru2')\n",
        "    encoder_out2, encoder_state2 = encoder_gru2(encoder_out1)\n",
        "    encoder_states = [encoder_state1, encoder_state2]\n",
        "\n",
        "    decoder_gru1 = GRU(2 * hidden_size, return_sequences=True, return_state=True, name='decoder_gru1')\n",
        "    decoder_out1, decoder_state1 = decoder_gru1(decoder_embedded, initial_state=encoder_state1)\n",
        "    decoder_gru2 = GRU(2 * hidden_size, return_sequences=True, return_state=True, name='decoder_gru2')\n",
        "    decoder_out2, decoder_state2 = decoder_gru2(decoder_out1, initial_state=encoder_state2)\n",
        "\n",
        "    attn_layer = AttentionLayer(name='attention_layer')\n",
        "    attn_out, attn_states = attn_layer([encoder_out2, decoder_out2])\n",
        "\n",
        "    decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_out2, attn_out])\n",
        "\n",
        "    dense=Dense(target_lang_vocab_size,activation='softmax',name='softmax_layer')\n",
        "    dense_time=TimeDistributed(dense,name='time_distributed_layer')\n",
        "    decoder_pred=dense_time(decoder_concat_input)\n",
        "\n",
        "    full_model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=decoder_pred)\n",
        "    full_model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "    full_model.summary(line_length=225)\n",
        "\n",
        "    encoder_model = Model(encoder_inputs, [encoder_out2]+encoder_states)\n",
        "    encoder_model.summary(line_length=225)\n",
        "\n",
        "    inf_decoder_state1 = Input(shape=(2*hidden_size,))\n",
        "    inf_decoder_state2 = Input(shape=(2*hidden_size,))\n",
        "    inf_decoder_inputs = Input(shape=(1,), name='decoder_inputs')\n",
        "    inf_encoder_outputs=Input(shape=(source_lang_timesteps,2*hidden_size,))\n",
        "    inf_decoder_embedded = decoder_embedding_layer(inf_decoder_inputs)\n",
        "\n",
        "    inf_decoder_out1, inf_decoder_state_out1= decoder_gru1(inf_decoder_embedded,initial_state=inf_decoder_state1)\n",
        "    inf_decoder_out2, inf_decoder_state_out2 = decoder_gru2(inf_decoder_out1, initial_state=inf_decoder_state2)\n",
        "    inf_attn_out, inf_attn_states = attn_layer([inf_encoder_outputs, inf_decoder_out2])\n",
        "    inf_decoder_concat = Concatenate(axis=-1, name='concat')([inf_decoder_out2, inf_attn_out])\n",
        "\n",
        "    inf_decoder_pred = TimeDistributed(dense)(inf_decoder_concat)\n",
        "    decoder_model = Model(inputs=[inf_encoder_outputs,inf_decoder_inputs,inf_decoder_state1,inf_decoder_state2],outputs=[inf_decoder_pred,inf_attn_states,inf_decoder_state_out1,inf_decoder_state_out2])\n",
        "    decoder_model.summary(line_length=225)\n",
        "    return full_model, encoder_model, decoder_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "17P--UN5dhwm"
      },
      "outputs": [],
      "source": [
        "from tensorflow.python.keras.utils.data_utils import Sequence\n",
        "from sklearn.utils import shuffle\n",
        "import numpy as np\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "class DataGenerator(Sequence):\n",
        "    def __init__(self,source_text,target_text,source_tokenizer,target_tokenizer,target_vocab_size,source_timesteps,target_timesteps,batch_size=32,shuffle=True):\n",
        "        self.source_text=source_text\n",
        "        self.target_text=target_text\n",
        "        self.source_tokenizer=source_tokenizer\n",
        "        self.target_tokenizer=target_tokenizer\n",
        "        self.target_vocab_size=target_vocab_size\n",
        "        self.source_timesteps=source_timesteps\n",
        "        self.target_timesteps=target_timesteps\n",
        "        self.batch_size=batch_size\n",
        "        self.shuffle=shuffle\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.source_text)/float(self.batch_size)))\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        if self.shuffle==True:\n",
        "            self.source_text,self.target_text=shuffle(self.source_text,self.target_text)\n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "        source_text=self.source_text[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        target_text=self.target_text[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        source_text_encoded = self.source_tokenizer.texts_to_sequences(source_text)\n",
        "        target_text_encoded = self.target_tokenizer.texts_to_sequences(target_text)\n",
        "        source_preproc_text = pad_sequences(source_text_encoded, padding='post', maxlen=self.source_timesteps)\n",
        "        target_preproc_text = pad_sequences(target_text_encoded, padding='post', maxlen=self.target_timesteps)\n",
        "        target_categorical=to_categorical(target_preproc_text,num_classes=self.target_vocab_size)\n",
        "        return [source_preproc_text,target_preproc_text[:,:-1]],target_categorical[:,1:,:]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "K3fr8LXjdjHi"
      },
      "outputs": [],
      "source": [
        "def translate(sentence,encoder_model,decoder_model,source_tokenizer,target_tokenizer,src_vsize,tgt_vsize,source_timesteps,target_timesteps):\n",
        "    target=\"sentencestart\"\n",
        "    source_text_encoded = source_tokenizer.texts_to_sequences([sentence])\n",
        "    target_text_encoded = target_tokenizer.texts_to_sequences([target])\n",
        "    source_preproc_text = pad_sequences(source_text_encoded, padding='post', maxlen=source_timesteps)\n",
        "    target_preproc_text=pad_sequences(target_text_encoded,padding='post',maxlen=1)\n",
        "    encoder_out,enc_last_state1,enc_last_state2=encoder_model.predict(source_preproc_text)\n",
        "    continuePrediction=True\n",
        "    output_sentence=''\n",
        "    total=0\n",
        "    while continuePrediction:\n",
        "        decoder_pred,attn_state,decoder_state1,decoder_state2=decoder_model.predict([encoder_out,target_preproc_text,enc_last_state1,enc_last_state2])\n",
        "        index_value = np.argmax(decoder_pred, axis=-1)[0, 0]\n",
        "        sTemp = target_tokenizer.index_word.get(index_value, 'UNK')\n",
        "        output_sentence += sTemp + ' '\n",
        "        total += 1\n",
        "        if total >= target_timesteps or sTemp == 'sentenceend':\n",
        "            continuePrediction = False\n",
        "        enc_last_state1=decoder_state1\n",
        "        enc_last_state2=decoder_state2\n",
        "        target_preproc_text[0,0]=index_value\n",
        "    return output_sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TRAIN SECTION"
      ],
      "metadata": {
        "id": "nENGxleyFuN9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "src_train, src_cv, src_test, tgt_train, tgt_cv, tgt_test, source_tokenizer, target_tokenizer = build_tokenizer_and_split_text(data, src_min_words=1, tgt_min_words=1)"
      ],
      "metadata": {
        "id": "dvlLdJZMw-X8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if source_tokenizer.num_words is None:\n",
        "  src_vsize = max(source_tokenizer.index_word.keys()) + 1\n",
        "else:\n",
        "  if (max(source_tokenizer.index_word.keys()) + 1) < source_tokenizer.num_words:\n",
        "    src_vsize = max(source_tokenizer.index_word.keys()) + 1\n",
        "  else:\n",
        "    src_vsize = source_tokenizer.num_words\n",
        "\n",
        "    \n",
        "if target_tokenizer.num_words is None:\n",
        "    tgt_vsize = max(target_tokenizer.index_word.keys()) + 1\n",
        "else:\n",
        "    if max(target_tokenizer.index_word.keys()) + 1 < target_tokenizer.num_words:\n",
        "      tgt_vsize = max(target_tokenizer.index_word.keys()) + 1\n",
        "    else:\n",
        "      tgt_vsize = target_tokenizer.num_words"
      ],
      "metadata": {
        "id": "oqZB3jh4GU9S"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "P7FjHcEGdk-D"
      },
      "outputs": [],
      "source": [
        "SOURCE_TIMESTEPS,TARGET_TIMESTEPS=20,20\n",
        "HIDDEN_SIZE=256\n",
        "EMBEDDING_DIM=100\n",
        "NUM_EPOCHS=1\n",
        "BATCH_SIZE=64\n",
        "DROPOUT=1.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "full_model, encoder_model, decoder_model = define_nmt(hidden_size=HIDDEN_SIZE, embedding_dim=EMBEDDING_DIM,\n",
        "                                                              source_lang_timesteps=SOURCE_TIMESTEPS,\n",
        "                                                              source_lang_vocab_size=src_vsize,\n",
        "                                                              target_lang_timesteps=TARGET_TIMESTEPS,\n",
        "                                                              target_lang_vocab_size=tgt_vsize, dropout=DROPOUT)"
      ],
      "metadata": {
        "id": "hGj6BGccxAjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_model.summary(line_length=225)\n",
        "encoder_model.summary(line_length=225)\n",
        "decoder_model.summary(line_length=225)"
      ],
      "metadata": {
        "id": "h9SvsAt84BDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-Lbtu8gdpyN"
      },
      "outputs": [],
      "source": [
        "training_generator=DataGenerator(source_text=src_train,target_text=tgt_train,source_tokenizer=source_tokenizer,target_tokenizer=target_tokenizer,\n",
        "                                         target_vocab_size=tgt_vsize,source_timesteps=SOURCE_TIMESTEPS,target_timesteps=TARGET_TIMESTEPS,batch_size=BATCH_SIZE,shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J48FKEWBdrFa"
      },
      "outputs": [],
      "source": [
        "validation_generator=DataGenerator(source_text=src_cv,target_text=tgt_cv,source_tokenizer=source_tokenizer,target_tokenizer=target_tokenizer,\n",
        "                                         target_vocab_size=tgt_vsize,source_timesteps=SOURCE_TIMESTEPS,target_timesteps=TARGET_TIMESTEPS,batch_size=BATCH_SIZE,shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = full_model.fit_generator(generator=training_generator,validation_data=validation_generator,use_multiprocessing=True,workers=6,epochs=NUM_EPOCHS)"
      ],
      "metadata": {
        "id": "34GUeE-W3-cH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GRAPH SECTION"
      ],
      "metadata": {
        "id": "0mtyYKwjAFzo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.plot(history.history['loss'], label=\"loss\")\n",
        "plt.plot(history.history['val_loss'], label=\"val_loss\")\n",
        "\n",
        "plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RqDBC2H_38Il"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SAVE AND LOAD SECTION"
      ],
      "metadata": {
        "id": "QuKWcBazAVq2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dir_hash='GRU_Attention'\n",
        "model_dict = {\n",
        "          'HiddenSize': HIDDEN_SIZE,\n",
        "          'EmbeddingDim': EMBEDDING_DIM,\n",
        "          'SourceTimeSteps': 20,\n",
        "          'TargetTimeSteps': 20,\n",
        "          'SourceVocab': src_vsize,\n",
        "          'TargetVocab': tgt_vsize,\n",
        "      }\n",
        "\n",
        "save_model(dir_hash, model_dict, full_model, encoder_model, decoder_model, source_tokenizer, target_tokenizer)"
      ],
      "metadata": {
        "id": "AZPUewLK33bM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model_dict, source_tokenizer, target_tokenizer, _, encoder_model, decoder_model = load_saved_model(dir_hash)\n",
        "\n",
        "src_vsize = model_dict['SourceVocab']\n",
        "tgt_vsize = model_dict['TargetVocab']\n",
        "SOURCE_TIMESTEPS = model_dict['SourceTimeSteps']\n",
        "TARGET_TIMESTEPS = model_dict['TargetTimeSteps']\n",
        "HIDDEN_SIZE = model_dict['HiddenSize']\n",
        "EMBEDDING_DIM = model_dict['EmbeddingDim']\n"
      ],
      "metadata": {
        "id": "qjOjJj-G31GR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BLEU SECTION"
      ],
      "metadata": {
        "id": "-1rpU3ne-dSy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate import bleu\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "\n",
        "total_bleu=0.0\n",
        "smoothie = SmoothingFunction().method4\n",
        "\n",
        "bleu_1=0.0\n",
        "bleu_2=0.0\n",
        "bleu_3=0.0\n",
        "bleu_4=0.0\n",
        "\n",
        "total_bleu_1=0.0\n",
        "total_bleu_2=0.0\n",
        "total_bleu_3=0.0\n",
        "total_bleu_4=0.0\n",
        "\n",
        "\n",
        "for i,sentence in enumerate(src_test):\n",
        "  translation=translate(sentence, encoder_model, decoder_model, source_tokenizer, target_tokenizer, src_vsize,tgt_vsize, SOURCE_TIMESTEPS, TARGET_TIMESTEPS)\n",
        "  expected=[tgt_test[i].replace(\"sentencestart \",\"\")]\n",
        "  translation=translation.replace(\" sentenceend\",\"\")\n",
        "  bleu=sentence_bleu(expected,translation, smoothing_function=smoothie)\n",
        "  bleu_1=sentence_bleu(expected,translation, smoothing_function=smoothie, weights=(1, 0, 0, 0))\n",
        "  bleu_2=sentence_bleu(expected,translation, smoothing_function=smoothie, weights=(0.5, 0.5, 0, 0))\n",
        "  bleu_3=sentence_bleu(expected,translation, smoothing_function=smoothie, weights=(0.33, 0.33, 0.33, 0))\n",
        "  bleu_4=sentence_bleu(expected,translation, smoothing_function=smoothie, weights=(0.25, 0.25, 0.25, 0.25))\n",
        "  total_bleu_1+=bleu_1\n",
        "  total_bleu_2+=bleu_2\n",
        "  total_bleu_3+=bleu_3\n",
        "  total_bleu_4+=bleu_4\n",
        "  total_bleu+=bleu\n",
        "\n",
        "total_bleu/=len(src_test)\n",
        "total_bleu_1/=len(src_test)\n",
        "total_bleu_2/=len(src_test)\n",
        "total_bleu_3/=len(src_test)\n",
        "total_bleu_4/=len(src_test)\n",
        "\n",
        "print(\"Average bleu score for \"+str(len(src_test))+\" items: \"+str(total_bleu))\n",
        "print(\"Average BLEU-1 for \"+str(len(src_test))+\" items: \"+str(total_bleu_1))\n",
        "print(\"Average BLEU-2 for \"+str(len(src_test))+\" items: \"+str(total_bleu_2))\n",
        "print(\"Average BLEU-3 for \"+str(len(src_test))+\" items: \"+str(total_bleu_3))\n",
        "print(\"Average BLEU-4 for \"+str(len(src_test))+\" items: \"+str(total_bleu_4))"
      ],
      "metadata": {
        "id": "Xg5iCCgK3zKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TEST SECTION"
      ],
      "metadata": {
        "id": "hdMYYMHR-1qM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "i = 0\n",
        "while i < 5:\n",
        "  sentence = input(\"Please enter a source word: \")\n",
        "  translation = translate(sentence, encoder_model, decoder_model, source_tokenizer, target_tokenizer, src_vsize,\n",
        "                                    tgt_vsize, SOURCE_TIMESTEPS, TARGET_TIMESTEPS)\n",
        "  print(\"Translation to target word \" + translation.replace(\" sentenceend\",\"\"))\n",
        "  i = i + 1"
      ],
      "metadata": {
        "id": "-9Ga5CfD3w6a"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "ShareThesisNMT.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}